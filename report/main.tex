\documentclass[a4paper,table]{article}

\usepackage[frenchb]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{a4wide}
\usepackage{enumitem}

\usepackage{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{steelblue}{rgb}{0.16,0.37,0.58}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{blue}{rgb}{0,0,0.7}
\definecolor{hlColor}{rgb}{0.94,0.94,0.94}
\definecolor{shadecolor}{rgb}{0.96,0.96,0.96}
\definecolor{TFFrameColor}{rgb}{0.96,0.96,0.96}
\definecolor{TFTitleColor}{rgb}{0.00,0.00,0.00}
\definecolor{lightred}{rgb}{1,0.96,0.96}
\definecolor{darkred}{rgb}{0.85,0.33,0.31}
\definecolor{lightblue}{HTML}{EBF5FA}
\definecolor{lightblue2}{HTML}{E3F2FA}
\definecolor{lightgreen}{HTML}{71E64A}
\definecolor{darkblue}{HTML}{D2DCE1}
\definecolor{lightyellow}{HTML}{FFFAE6}
\definecolor{darkyellow}{HTML}{FAE6BE}

\usepackage{listings}
\lstset{
	language=C,
	basicstyle=\scriptsize,
	numbers=left,                   % where to put the line-numbers
  	numberstyle=\tiny\color{gray},
	commentstyle=\color{steelblue},
	stringstyle=\color{darkred},
	backgroundcolor=\color{shadecolor},
    keywordstyle=\color{dkgreen},
	frame=single,                   % adds a frame around the code
 	rulecolor=\color{black},
	emph={},
	emphstyle=\color{mauve},
	morekeywords=[2]{},
	keywordstyle=[2]{\color{dkgreen}},
	showstringspaces=false,
  	tabsize=4,
	moredelim=[is][\small\ttfamily]{/!}{!/},
	breaklines=true
}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true, % false: boxed links; true: colored links
	linkcolor=black, % color of internal links
	urlcolor=blue,   % color of external links
	citecolor=blue
}
\newcommand{\hhref}[1]{\href{#1}{#1}}

\usepackage{makecell}

\usepackage{eurosym} %\euro -> €

\usepackage{soul}
\sethlcolor{hlColor}

\title{INF5101A - TP2 MPI}

\date{\today}

\begin{document}
\maketitle
\newpage

\section{Calcul de $\pi$}

Dans cet exercice, l'objectif est de calculer une valeur la plus proche de
$\pi$. Cette valeur peut s'obtenir avec un calcul d'intégrale. Par conséquent,
pour atteindre la plus grande précision, il faudra effectuer les calculs avec
des très petits pas entre les bornes de l'intervalle. En procédant de cette
façon, le calcul de la valeur de $\pi$ demandera beaucoup de temps. A l'aide de
la bibliothèque MPI, nous pouvons paralléliser ces calculs sur un nombre de
processeurs que nous choisissons. \\

Pour implémenter cette théorie, nous avons fait en sorte que chaque processeur
qui exécute le programme se détermine des bornes d'intervalle en fonction de
leur numéro de tâche. Lorsque tous les calculs sont terminés, nous utilisons la
fonction \hl{MPI\_Reduce()} qui est ici configurée pour calculer sur le
processeur 0 la somme de toutes les valeurs obtenues par chaque processeur.

\section{Structure de données en parallèle}

Dans l'exécution d'algorithme avec des matrices, nous avons tendance à diviser
les données entre les processeurs qui participent à la parallélisation.
Cependant, il arrive que ceux-ci aient besoin de données voisines stockées sur
les autres processeurs. C'est pourquoi nous faisons du recouvrement de données.

Dans cet exercice, nous faisons une décomposition 1D d'une matrice sur plusieurs
processus. Une fois que cela est fait, deux lignes sont ajoutées autour
de cette matrice et représentent la dernière ligne du processeur de numéro
précédent dans le classement des tâches et la première ligne du processeur de
numéro suivant. C'est le recouvrement de données. \\

\begin{lstlisting}
typedef struct LocalMatrix {
    double* beforeLine;
    double** matrix;
    double* afterLine;

    int innerLines;
    int totalLines;
    int cols;
} LocalMatrix;
\end{lstlisting}
\

Pour mettre à jour les lignes de recouvrement, le programme utilise
\hl{MPI\_Send()} pour envoyer un buffer de données avec un type choisi
(e.g. \hl{MPI\_DOUBLE}) à un destinataire, puis \hl{MPI\_Recv()} pour recevoir
les données qui lui sont destinées. Ces envois et réceptions ont été regroupés
dans la fonction \hl{updateBoundaries()}. \\

Après un ensemble d'actions parallèles avec ces fonctions, nous pouvons nous
assurer à ce que les programmes lancés parallèlement se rejoignent en un même
point dans le déroulement du programme avec la fonction \hl{MPI\_Barrier()}.
Elle fait attendre les différentes tâches jusqu'à ce que toutes les tâches
arrivent à cette barrière. \\

L'ordre d'arrivé des messages sur la sortie standard n'étant pas garantie en
parallèle en utilisant \hl{printf()}, nous avons implémenté une fonction
\hl{writeFullMatrix()} afin de pouvoir écrire la matrice dans un fichier.
Le processus ayant le rang 0 écrit tout d'abord sa matrice dans le fichier
puis envoie un jeton au processus suivant. \hl{MPI\_Recv()} étant bloquant,
chaque processus attend de recevoir le jeton avant de pouvoir écrire dans le
fichier.
\newpage

\section{Équation de Laplace}

Nous avons implémenté la résolution de l'équation de Laplace en parallèle en
utilisant la structure de données définie dans la section précédente.\\

Tant que la convergence n'est pas atteinte, tous les pixels de la matrice
locale sont parcourus (à l'exception des pixels sur les bords) et pour chacun
d'entre eux une nouvelle valeur est calculée à partir des 4 pixels adjacents.
La convergence est atteinte lorsque l'erreur est plus faible qu'un paramètre
\hl{delta} défini au préalable. A la fin de chaque parcours complet de la
matrice, les lignes de recouvrement sont mises à jour.\\

Notre première approche avait été de laisser chaque processus converger à des
moments différents, et utiliser un système de jetons afin de prévenir les
processus adjacents de la convergence d'un processus. Cette approche avait
pour désavantage d'alourdir la syntaxe, et de se retrouver avec des cas
particuliers pénibles à gérer au niveau des envois et réceptions (e.g.
P0 converge, envoie ses lignes de recouvrement et envoie un jeton prévenant
les processus adjacents de sa convergence. P1 reçoit le jeton et ignore la
nouvelle ligne de convergences de P0.\\

La seconde approche, qui est actuellement utilisée, est de se servir de
\hl{MPI\_AllReduce()} afin de calculer sur tous les processus la somme des
erreurs de tous les processus. Ainsi, les processus convergeront au même
moment.

\begin{lstlisting}
while (err > delta) {
	// compute one step of Laplace's equation
	
    double tmpErr;
    MPI_Allreduce(&err, &tmpErr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
    err = sqrt(tmpErr);
	
	// update boundaries
}
\end{lstlisting}

\section{Décomposition 2D}

Dans le cas précédent (décomposition 1D), chaque processus possédait une matrice
$\frac{nmatrix}{nprocs}$ lignes \& $nmatrix$ colonnes. Dans le cas de la
décomposition 2D, chaque processus possède une matrice carré de taille
$\frac{nmatrix}{\sqrt{nprocs}}$.\\

De plus, 2 colonnes de recouvrement ont été ajoutées en plus des 2 lignes de
recouvrement précédentes. Cela implique qu'il faille des communications
supplémentaires pour mettre à jour la zone de recouvrement (2 * \hl{MPI\_Send()}
\& 2 * \hl{MPI\_Recv()} supplémentaires). Dans le cas de la décomposition 1D,
chaque processus envoyait $2 * nmatrix$ éléments (2 lignes). Pour la
décomposition 2D, nous envoyons $4 * \frac{nmatrix}{\sqrt{nprocs}}$ éléments,
(2 lignes \& 2 colonnes). La décomposition 2D nous permet donc de diminuer le
nombre d'éléments à envoyer lorsque le nombre de processus augmente, ce qui
n'était pas le cas de la décomposition 1D.\\

Pour mettre à jour les colonnes de recouvrement, il faut tout d'abord envoyer
la première colonne de la matrice locale au processus de gauche, et la dernière
colonne au processus de droite. Envoyer une colonne ne peut pas être effectué
directement, sachant que nous travaillons en \emph{row-major order}. Plutôt que
d'effectuer un \hl{MPI\_Send()} pour chaque élément de la colonne, nous pouvons
définir un nouveau type à l'aide de \hl{MPI\_Type\_vector()}.\\
\newpage

\begin{lstlisting}
MPI_Datatype col;
MPI_Type_vector(
    localMatrix.innerSize, // number of blocks
    1,                     // number of elements per block
    localMatrix.innerSize, // stride
    MPI_DOUBLE, &col
);
MPI_Type_commit(&col);
\end{lstlisting}
\

Le listing ci-dessus décrit un type de donnée comportant
\hl{localMatrix.innerSize} blocs de 1 élément, sachant que chaque bloc sera
séparé par \hl{localMatrix.innerSize} éléments. Nous avons donc décrit un
type représentant une colonne.\\

Cela n'est cependant pas suffisant. Nous stockons les colonnes de recouvrements
(\hl{localMatrix.beforeCol} \& \hl{localMatrix.afterCol}) sous forme de lignes.
Si nous utilisons le type précédemment créé, nous envoyons les colonnes avec
le type \hl{col} et nous sommes obligés de recevoir un élément de type \hl{col}
également. Procéder ainsi nécessiterait une étape supplémentaire après
réception des données (réception d'une colonne dans un buffer puis copie des
éléments de la colonne dans la colonne de recouvrement).
\hl{MPI\_Type\_create\_resized()} permet de définir un nouveau type à partir du
type précédemment créé, et de définir l'espacement entre les éléments.\\

\begin{lstlisting}
MPI_Type_create_resized(col, 0, 1*sizeof(double), &ColType);
MPI_Type_commit(&ColType);
\end{lstlisting}
\

Nous pouvons donc maintenant envoyer une colonne de type \hl{ColType}, et
recevoir \hl{localMatrix.innerSize} éléments de type \hl{MPI\_DOUBLE}
(une ligne).

\section{Benchmark}

\begin{table}[h!]
\begin{tabular}{|l|c|c|c|l|}
\hline
\multicolumn{5}{|c|}{Pi}                                       \\ \hline
\diaghead{aaaaaaaaaa}{interv.}{nprocs} & 1            & 4           & 9           & 36         \\ \hline
1.0e7  & 0.111315 s   & 0.030315 s  & 0.017002 s  & \cellcolor{lightgreen}0.008932 s \\ \hline
1.0e8  & 1.110433 s   & 0.279610 s  & 0.126984 s  & \cellcolor{lightgreen}0.066226 s \\ \hline
1.0e9  & 11.100553 s  & 2.778888 s  & 1.236657 s  & \cellcolor{lightgreen}0.653338 s \\ \hline
1.0e10 & 110.997922 s & 27.758928 s & 12.344067 s & \cellcolor{lightgreen}6.510308 s \\ \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\begin{tabular}{|l|c|c|c|l|}
\hline
\multicolumn{5}{|c|}{Laplace 1D}                              \\ \hline
\diaghead{aaaaaaaaaaaa}{nmatrix}{nprocs} & 1           & 4           & 9            & 36           \\ \hline
36  & \cellcolor{lightgreen}0.111793 s  & 1.488305 s                        & 4.644088 s   & 27.630346 s  \\ \hline
72  & \cellcolor{lightgreen}1.864976 s  & 6.440377 s                        & 17.235972 s  & 96.150103 s  \\ \hline
108 & \cellcolor{lightgreen}9.300971 s  & 15.787466 s                       & 39.982225 s  & 220.966194 s \\ \hline
144 & \cellcolor{lightgreen}29.476426 s & 31.491000 s                       & 72.280533 s  & 398.455938 s \\ \hline
180 & 70.525075 s                       & \cellcolor{lightgreen}54.945528 s & 137.704287 s & 684.308997 s \\ \hline
\end{tabular}
\end{table}
\newpage

\begin{table}[h!]
\begin{tabular}{|l|c|c|c|l|}
\hline
\multicolumn{5}{|c|}{Laplace 2D}                                \\ \hline
\diaghead{aaaaaaaaaaaa}{nmatrix}{nprocs} & 1            & 4            & 9            & 36           \\ \hline
36  & \cellcolor{lightgreen}0.219014 s   & 1.084970 s                         & 2.238108 s   & 2.762289 s   \\ \hline
72  & \cellcolor{lightgreen}3.657851 s   & 4.937405 s                         & 8.808075 s   & 14.282993 s  \\ \hline
108 & 18.342742 s                        & \cellcolor{lightgreen}13.659933 s  & 20.358381 s  & 29.913704 s  \\ \hline
144 & 57.801533 s                        & \cellcolor{lightgreen}30.768816 s  & 38.485890 s  & 51.611273 s  \\ \hline
180 & 139.236856 s                       & \cellcolor{lightgreen}63.491195 s  & 65.181726 s  & 86.970718 s  \\ \hline
216 & 289.749480 s                       & \cellcolor{lightgreen}113.419590 s & 138.083669 s & 156.648471 s \\ \hline
252 & 526.033825 s                       & \cellcolor{lightgreen}186.899420 s & 201.120697 s & 213.800121 s \\ \hline
\end{tabular}
\end{table}

Pour les tailles de matrice testées, Laplace 1D obtient les meilleurs
temps de calcul. Cependant, Laplace 2D obtient des temps de calcul plus faibles
que la version 1D lorsque le nombre de processus est plus grand. Cette dernière
observation s'explique par le nombre d'éléments à envoyer diminuant lorsque le
nombre de processus augmente dans le cas de la décomposition 2D.

\end{document}
